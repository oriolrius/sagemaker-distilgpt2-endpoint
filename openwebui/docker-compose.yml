services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "49201:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}
      - AWS_REGION_NAME=${AWS_REGION_NAME:-eu-north-1}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
    command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "49200:8080"
    volumes:
      - ./data/openwebui:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - WEBUI_AUTH=false
      - ENABLE_OLLAMA_API=false
    depends_on:
      - litellm
    restart: unless-stopped
